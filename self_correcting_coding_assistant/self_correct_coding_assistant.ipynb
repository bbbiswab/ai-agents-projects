{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Required Imports",
   "id": "eecfcb093ebcd9b0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:42.956399600Z",
     "start_time": "2026-02-11T00:54:42.926756800Z"
    }
   },
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import subprocess                                   # Run python scripts as process\n",
    "import textwrap                                     # Indentation for generated code\n",
    "import tempfile                                     # Create temporary files\n",
    "\n",
    "# Langgraph and typing imports\n",
    "from typing import TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, END         # Agent graph orchestration\n",
    "\n",
    "# Google API\n",
    "from google import genai                            # Google GenAI client\n"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Gemini Client",
   "id": "ee736e750ad78cf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:43.007166500Z",
     "start_time": "2026-02-11T00:54:42.966404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting the google api in the environment\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD6YPLUfxWJVsECioR5GBaT_9j_8xd_jEQ\""
   ],
   "id": "f2cee694c291a30c",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.047008800Z",
     "start_time": "2026-02-11T00:54:43.023170400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Gemini Client using the api key from the environment variables\n",
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Listing the available models for use\n",
    "print(\"Available Gemini models:\")\n",
    "for model in client.models.list():\n",
    "    print(f\"* **Name:** {model.name}\")\n",
    "    print(f\"  **Description:** {model.description}\")"
   ],
   "id": "283fb5d9b13fad0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models:\n",
      "* **Name:** models/gemini-2.5-flash\n",
      "  **Description:** Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.\n",
      "* **Name:** models/gemini-2.5-pro\n",
      "  **Description:** Stable release (June 17th, 2025) of Gemini 2.5 Pro\n",
      "* **Name:** models/gemini-2.0-flash\n",
      "  **Description:** Gemini 2.0 Flash\n",
      "* **Name:** models/gemini-2.0-flash-001\n",
      "  **Description:** Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "* **Name:** models/gemini-2.0-flash-exp-image-generation\n",
      "  **Description:** Gemini 2.0 Flash (Image Generation) Experimental\n",
      "* **Name:** models/gemini-2.0-flash-lite-001\n",
      "  **Description:** Stable version of Gemini 2.0 Flash-Lite\n",
      "* **Name:** models/gemini-2.0-flash-lite\n",
      "  **Description:** Gemini 2.0 Flash-Lite\n",
      "* **Name:** models/gemini-exp-1206\n",
      "  **Description:** Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "* **Name:** models/gemini-2.5-flash-preview-tts\n",
      "  **Description:** Gemini 2.5 Flash Preview TTS\n",
      "* **Name:** models/gemini-2.5-pro-preview-tts\n",
      "  **Description:** Gemini 2.5 Pro Preview TTS\n",
      "* **Name:** models/gemma-3-1b-it\n",
      "  **Description:** None\n",
      "* **Name:** models/gemma-3-4b-it\n",
      "  **Description:** None\n",
      "* **Name:** models/gemma-3-12b-it\n",
      "  **Description:** None\n",
      "* **Name:** models/gemma-3-27b-it\n",
      "  **Description:** None\n",
      "* **Name:** models/gemma-3n-e4b-it\n",
      "  **Description:** None\n",
      "* **Name:** models/gemma-3n-e2b-it\n",
      "  **Description:** None\n",
      "* **Name:** models/gemini-flash-latest\n",
      "  **Description:** Latest release of Gemini Flash\n",
      "* **Name:** models/gemini-flash-lite-latest\n",
      "  **Description:** Latest release of Gemini Flash-Lite\n",
      "* **Name:** models/gemini-pro-latest\n",
      "  **Description:** Latest release of Gemini Pro\n",
      "* **Name:** models/gemini-2.5-flash-lite\n",
      "  **Description:** Stable version of Gemini 2.5 Flash-Lite, released in July of 2025\n",
      "* **Name:** models/gemini-2.5-flash-image\n",
      "  **Description:** Gemini 2.5 Flash Preview Image\n",
      "* **Name:** models/gemini-2.5-flash-preview-09-2025\n",
      "  **Description:** Gemini 2.5 Flash Preview Sep 2025\n",
      "* **Name:** models/gemini-2.5-flash-lite-preview-09-2025\n",
      "  **Description:** Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite\n",
      "* **Name:** models/gemini-3-pro-preview\n",
      "  **Description:** Gemini 3 Pro Preview\n",
      "* **Name:** models/gemini-3-flash-preview\n",
      "  **Description:** Gemini 3 Flash Preview\n",
      "* **Name:** models/gemini-3-pro-image-preview\n",
      "  **Description:** Gemini 3 Pro Image Preview\n",
      "* **Name:** models/nano-banana-pro-preview\n",
      "  **Description:** Gemini 3 Pro Image Preview\n",
      "* **Name:** models/gemini-robotics-er-1.5-preview\n",
      "  **Description:** Gemini Robotics-ER 1.5 Preview\n",
      "* **Name:** models/gemini-2.5-computer-use-preview-10-2025\n",
      "  **Description:** Gemini 2.5 Computer Use Preview 10-2025\n",
      "* **Name:** models/deep-research-pro-preview-12-2025\n",
      "  **Description:** Preview release (December 12th, 2025) of Deep Research Pro\n",
      "* **Name:** models/gemini-embedding-001\n",
      "  **Description:** Obtain a distributed representation of a text.\n",
      "* **Name:** models/aqa\n",
      "  **Description:** Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
      "* **Name:** models/imagen-4.0-generate-preview-06-06\n",
      "  **Description:** Vertex served Imagen 4.0 model\n",
      "* **Name:** models/imagen-4.0-ultra-generate-preview-06-06\n",
      "  **Description:** Vertex served Imagen 4.0 ultra model\n",
      "* **Name:** models/imagen-4.0-generate-001\n",
      "  **Description:** Vertex served Imagen 4.0 model\n",
      "* **Name:** models/imagen-4.0-ultra-generate-001\n",
      "  **Description:** Vertex served Imagen 4.0 ultra model\n",
      "* **Name:** models/imagen-4.0-fast-generate-001\n",
      "  **Description:** Vertex served Imagen 4.0 Fast model\n",
      "* **Name:** models/veo-2.0-generate-001\n",
      "  **Description:** Vertex served Veo 2 model. Access to this model requires billing to be enabled on the associated Google Cloud Platform account. Please visit https://console.cloud.google.com/billing to enable it.\n",
      "* **Name:** models/veo-3.0-generate-001\n",
      "  **Description:** Veo 3\n",
      "* **Name:** models/veo-3.0-fast-generate-001\n",
      "  **Description:** Veo 3 fast\n",
      "* **Name:** models/veo-3.1-generate-preview\n",
      "  **Description:** Veo 3.1\n",
      "* **Name:** models/veo-3.1-fast-generate-preview\n",
      "  **Description:** Veo 3.1 fast\n",
      "* **Name:** models/gemini-2.5-flash-native-audio-latest\n",
      "  **Description:** Latest release of Gemini 2.5 Flash Native Audio\n",
      "* **Name:** models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "  **Description:** Gemini 2.5 Flash Native Audio Preview 09-2025\n",
      "* **Name:** models/gemini-2.5-flash-native-audio-preview-12-2025\n",
      "  **Description:** Gemini 2.5 Flash Native Audio Preview 12-2025\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Share State Between Models",
   "id": "bf014aaabc84be34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.071780800Z",
     "start_time": "2026-02-11T00:54:44.062706300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "\n",
    "class CodeState(TypedDict):\n",
    "    prompt: str                             # User coding request\n",
    "    code : str                              # generated python code\n",
    "    output : Optional[str]                  # Successful execution of output\n",
    "    error : Optional[str]                   # Traceback if the code failed\n",
    "    success : bool                          # Success Indicator\n",
    "    attempts : int                          # How many tries so far"
   ],
   "id": "cee12cabd11c70aa",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.083128100Z",
     "start_time": "2026-02-11T00:54:44.071780800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper Function\n",
    "def extract_code(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes markdown code fences and returns pure code.\n",
    "    \"\"\"\n",
    "    # If fenced block exists\n",
    "    match = re.search(r\"```(?:python)?(.*?)```\", text, re.S | re.I)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # Otherwise return full text\n",
    "    return text.strip()"
   ],
   "id": "d9aeeb417518b57a",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LLM Call",
   "id": "88db859fdf1c7061"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.058405Z",
     "start_time": "2026-02-11T00:54:44.047008800Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 95,
   "source": [
    "#\n",
    "def llm_generate(prompt: str) -> str:\n",
    "    '''\n",
    "    Sends prompt to the Gemini and returns a text response.\n",
    "    '''\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"temperature\": 0.2,                     # Low Randomness (Stable code)\n",
    "            \"max_output_tokens\": 800                # Script length\n",
    "        }\n",
    "    )\n",
    "    return response.text"
   ],
   "id": "1a7c3f0b696abd16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nodes",
   "id": "3928423e185b520d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Node 1: Coder (LLM writes/fixes)",
   "id": "fcfd7bce8450504e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.094679700Z",
     "start_time": "2026-02-11T00:54:44.083128100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def coder(state: CodeState):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates or repairs code based on prompt + errors.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a senior Python engineer\n",
    "Write a Complete Python script that solves:\n",
    "\n",
    "{state.get('prompt')}\n",
    "\n",
    "If there was an error before, fix it.\n",
    "\n",
    "Previous error:\n",
    "{state.get('error')}\n",
    "\n",
    "Return only raw Python code. No explanation and do not use markdown.\n",
    "\"\"\"\n",
    "\n",
    "    raw = llm_generate(prompt)\n",
    "\n",
    "    code = extract_code(raw)\n",
    "\n",
    "    print(f'Generated code:\\n{code}')\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"code\": code,\n",
    "        \"error\": None,\n",
    "        \"output\": None,\n",
    "        \"success\": False,\n",
    "        \"attempts\": state.get(\"attempts\", 0) + 1,\n",
    "    }"
   ],
   "id": "9fdcbb2db0e2fe16",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Node 2: Executor (Run Python code)",
   "id": "3c0ee5bba420c354"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.109954900Z",
     "start_time": "2026-02-11T00:54:44.094679700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def executor(state: CodeState):\n",
    "\n",
    "    \"\"\"\n",
    "    Saves the code in a tempfile and executes it.\n",
    "    \"\"\"\n",
    "\n",
    "    # indentation for generated script\n",
    "    code = textwrap.dedent(state['code'])\n",
    "\n",
    "    # create temporary python file\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        suffix=\".py\",\n",
    "        delete=False,\n",
    "        mode=\"w\",\n",
    "    ) as f:\n",
    "        f.write(code)\n",
    "        filename = f.name\n",
    "\n",
    "    try:\n",
    "        # run the python file\n",
    "        result = subprocess.run([\"python\", filename],\n",
    "                                capture_output=True,\n",
    "                                text=True,\n",
    "                                timeout=10\n",
    "        )\n",
    "\n",
    "        # script run successfully\n",
    "        if result.returncode == 0:\n",
    "            return {\n",
    "                \"output\": result.stdout,\n",
    "                \"success\": True,\n",
    "                \"error\": None,\n",
    "            }\n",
    "\n",
    "        # script run failed\n",
    "        else:\n",
    "            return {\n",
    "                \"error\": result.stderr,\n",
    "                \"success\": False,\n",
    "            }\n",
    "\n",
    "    # catches if any system error\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"success\": False,\n",
    "        }\n"
   ],
   "id": "d6a0f54078ecf77c",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Node 3: Validator Node (decides loop)",
   "id": "5c8c74917d78a068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.118134100Z",
     "start_time": "2026-02-11T00:54:44.109954900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def validator(state: CodeState):\n",
    "\n",
    "    \"\"\"\n",
    "    Check success and controls retry loop\n",
    "    \"\"\"\n",
    "\n",
    "    # if code run successfully\n",
    "    if state[\"success\"]:\n",
    "        print(\"\\n success output: \\n\")\n",
    "        print(state[\"output\"])\n",
    "        return state\n",
    "\n",
    "    # Stop infinite retry of loops\n",
    "    if state[\"attempts\"] >= 5:\n",
    "        raise RuntimeError(\"Agent stuck after 5 attempts\")\n",
    "\n",
    "    # show error and retry\n",
    "    print(\"\\n Error - retrying:\\n\")\n",
    "    print(state[\"error\"])\n",
    "\n",
    "    return state\n"
   ],
   "id": "df5d2b08c292c0af",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Routing Logic (success vs retry)",
   "id": "573bb0194033c0f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.131944800Z",
     "start_time": "2026-02-11T00:54:44.118134100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def route(state: CodeState):\n",
    "\n",
    "    \"\"\"\n",
    "    If successful, end graph otherwise return to coder\n",
    "    \"\"\"\n",
    "\n",
    "    if state[\"success\"]:\n",
    "        return END\n",
    "    return \"coder\""
   ],
   "id": "e589e58a22159a50",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Langgraph Workflow",
   "id": "d4ba5b84dfd0f4b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:44.146405700Z",
     "start_time": "2026-02-11T00:54:44.133945900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph = StateGraph(CodeState)                       # Create state machine\n",
    "graph.add_node(\"coder\", coder)                      # Add Coder agent\n",
    "graph.add_node(\"executor\", executor)\n",
    "graph.add_node(\"validator\", validator)\n",
    "\n",
    "graph.set_entry_point(\"coder\")                      # Start point with coder\n",
    "\n",
    "graph.add_edge(\"coder\", \"executor\")\n",
    "graph.add_edge(\"executor\", \"validator\")\n",
    "\n",
    "# Add conditional Loop\n",
    "graph.add_conditional_edges(\"validator\", route)\n",
    "\n",
    "# Compile graph into runnable app\n",
    "app = graph.compile()"
   ],
   "id": "ffb706bed6d30043",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run the agent",
   "id": "488a421025b0826"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:46.616676100Z",
     "start_time": "2026-02-11T00:54:44.148407500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = app.invoke(\n",
    "        {\n",
    "            \"prompt\": \"Write a python code that prints prime numbers from 1 to 100\",\n",
    "            \"code\": \"\",\n",
    "            \"output\": None,\n",
    "            \"success\": False,\n",
    "            \"attempts\": 0\n",
    "        }\n",
    "    )"
   ],
   "id": "1999d6a01648aaa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated code:\n",
      "for num in range(2, 101):\n",
      "    is_prime = True\n",
      "    if num < 2:\n",
      "        is_prime = False\n",
      "    else:\n",
      "        for i in range(2, int(num**0.5) + 1):\n",
      "            if num % i == 0:\n",
      "                is_prime = False\n",
      "                break\n",
      "    if is_prime:\n",
      "        print(num)\n",
      "\n",
      " success output: \n",
      "\n",
      "2\n",
      "3\n",
      "5\n",
      "7\n",
      "11\n",
      "13\n",
      "17\n",
      "19\n",
      "23\n",
      "29\n",
      "31\n",
      "37\n",
      "41\n",
      "43\n",
      "47\n",
      "53\n",
      "59\n",
      "61\n",
      "67\n",
      "71\n",
      "73\n",
      "79\n",
      "83\n",
      "89\n",
      "97\n",
      "\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:54:46.637957500Z",
     "start_time": "2026-02-11T00:54:46.630959Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8dd5a42833863c28",
   "outputs": [],
   "execution_count": 103
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
